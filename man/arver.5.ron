arver(5) -- LUKS on the loose
========================================

## SYNOPSIS

    arver [-u user] [-c arverdata] [OPTIONS] ACTION [USER] [TARGET]
    
## DESCRIPTION

arver is a tool that helps you to manage large amount of encrypted harddisks
over mutliple servers and locations. Furthermore, it helps you to mange
access policies to encrypted harddisks of a bunch of people.

By default `arver` requires a specific action and (for most actions) a 
target to work. 

## OPTIONS

There are some generic command line options:

  * `-u USER`, `--user USER`:
    By default `arver` will read `~/.arver` to get your username. Using `-u USER` you can override it.
  * `-c PATH`, `--config-dir PATH`:
    By default `arver` will assume your data storage in `~/.arverdata`. However,
    if you have multiple data storage or you want to put it to a different
    location you can use the `-c PATH` option.

For more options see `arver -h`

## ACTIONS

The following actions are supported:

  * `--create TARGET`:
    Creates LUKS partitions for `arver` on all targeted disks.
  * `-o`, `--open TARGET`:
    Opens all targeted disks.
  * `--close TARGET`:
    Closes all targeted disks.
  * `-a`, `--add-user USER TARGET`:
    Adds permissions for USER on all targeted disks.
  * `-d`, `--del-user USER TARGET`:
    Removes permissions for USER on all targeted disks.
  * `-r`, `--refresh TARGET`:
    Refresh the key on the target.
  * `-i`, `--info TARGET`:
    Display the LUKS configuration of all targeted disks.
  * `-l`, `--list-targets`:
    List all possible targets.
  * `-k`, `--keys TARGET`:
    List available keys for the target.
  * `-g`, `--garbage-collect`:
    Cleans old entries from your `arver` keys.

## TARGETS

All Targets are defined in the `disks` config file. See the section
`Disks` for more details. A complete target looks like this:

    /location_name/host_name/disk_name

But the TARGET option accepts also partial names and lists. E.g 

    location1,location2/host2,host4,disk3

targets all disks at location1, all disks on host2 at location2, all disks on 
host4 and disk3. If any of the provided target names are not unique `arver` will
quit with an error message stating all matching targets.

To run an action on all possible disks use `ALL` as target.

## Working with arver

Working with arver is quite simple and straight forward. Within the next
paragraphs you'll find a detailed overview on the concept of arver,
as well as how to set it up and what the different actions are doing exactly. 

## Concept

To setup arver we only need to define in the corresponding configuration files our
(admin-)users and our disks. See section `Bootstrapping a new arverdata directory`
on how to setup your config.
 
Arver's configuration files are contained in a single directory which contains all
the necessary (non-private) information to manage your disks. This directory will
be referred to as `arverdata`. We recommend you to share that directory among your
group with a distributed version control system such as git.

Eeach admin will be assigned one global LUKS slot. Arver will use this information
to grant or revoke privileges to other users. See the `Managing users` section
for a detailed descripton of how this is done.

The `arverdata` directory contains the following files and directories:

    keys/            <- contains the users `arver` keyrings
    users            <- yaml file containing all users configuration (see `Users`)
    disks            <- yaml file containing all disk configuration (see `Disks`)
    keys/public/     <- contains gpg public keys of the admins (managed by arver)

## Bootstrapping a new arverdata directory

To bootstrap a new `arverdata` directory just run:

arver -u <new_username> --init

This will create ~/.arverdata with a sample configuration. See the next two sections
on how to adapt it to your setup:

### Users

The `user` config file contains all your users. It is very important that all users
use exactly the same configuration! It has the following structure:

    me: 
      slot: 1
      gpg: BEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFE
    foo:
      slot 2
      gpg: AFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAF

`me` is the identifier you selected on `--init`. `slot` referes to the LUKS slot which
will be used. This has to be a unique number between 0 and 7. If you'd like 
to migrate existing LUKS devices take care to avoid the currently used slot number
(usually 0). `gpg` is the (40 digit) fingerprint of the gpg-key. We recommend to use 
a dedicated gpg key just for `arver`. You can create one with:

gpg --gen-key

And you can find out the fingerprint of your key with:

gpg --fingerprint --list-key <key-id>

### Disks

The `disks` file contains the following hash tree in yaml notation:

     'hostgroup1':
       'host1':
         'address' : 'host1.example.com'
         'pre_open': 'pre_open_host_script.sh'
         'disk1'   :
           'device'   : 'storage/disk1'
           'post_open': 'post_open_disk_script.sh'
         'disk2'   : 
           'device'   :  'sdb1'
       'host2':
         'address' : 'host2.example.com'
         'port'    : '2222'
         'username': 'hans'
         'mails'   : 
           'device'  : 'nonraid/mails'
           'pre_open': 'pre_open_disk_script.sh'
     'hostgroup2':
       'host3':
         'address' : 'host3.example.com'
         'username': '#arveruser'
         'secure'  : 
           'device'  : 'storage/secure'
         
As you can see this allows you to organize your disks and servers in a tree
structure, which will enable you to manage your disks more efficiently within 
the later commands.

`hostgroup1` and `hostgroup2` is just a logical container which can contain any
amount of hosts. You can name them as you like. This is interesting if you have 
for example multiple hosts in one location and you need to quickly recover from 
a power outage from this location. 

Invoking

    arver --list-targets

will present you the tree of the various targets in your `disks` configuration 
file.

`host1`, `host2` and `host3` are identifiers for different hosts. These host-
objects can contain multiple disks and can have further information such as the
`address` of the host or the ssh-`port` number if the ssh daemon is not running
on the standart port.

`username` defines the ssh login-user. By default it is `root`. To always use
the same username as arver (as defined by `-u` or in `.arver`) set the
username to `#arveruser`. If the user is not `root`, the actual LUKS commands 
will be executed via `sudo` and you need the following entry in your sudoers 
file on this machine (assuming the user is in the admin group):

    %admin ALL=NOPASSWD: /usr/bin/test, /sbin/cryptsetup

You can also add script hooks to any host or disk. Those will be run during the
`open` and `close` actions at the appropriate time. The possible options are:
`pre_open`, `pre_close`, `post_open` and `post_close`. 

Any other entry within the hosts-object are actual disks entries of that particular
host. These disks are represented by an identifier and at least a `device` entry
pointing to the actual disk path. So for example the disks on `host1` are:
`/dev/storage/disk1` identified by `disk1` and `/dev/sdb1` idetified by `disk2`.
The prefix `/dev/` is alays added to the disk path.

## Action Create

To initially create an arver managed LUKS device you first need to add the 
device to the disks config. See above for various examples.
You can create the LUKS device by invoking the following command:

    $ arver --create TARGET 

What's happening behind the scene?

Arver creates a new LUKS device with a random password in your LUKS slot on the 
server. The password is then encrypted with your public key (defined in `users`)
and stored in `arverdata/keys/USERNAME/xxxx`

## Action Open

To open a LUKS device managed by arver you can invoke the `--open` action on any
target:

    $ arver --open TARGET

arver retrieves the password by decrypting the keys in data/keys/YOURUSERNAME
and uses this to open the LUKS device on the server.

See the section `TARGET` on how to open multiple disks at once.

You can define script hooks to be executed before and after the open command.
See `Disks` for details. The hooks are run in the following order:

  * pre_open of host
  *  pre_open of disk1
  *   open disk1
  *  post_open of disk1
  *  pre_open of disk2
  *   open disk2
  *  post_open of disk1
  * post_open of host

Those scripts have to be present at the actual host.

If you don't have a key for any of the disks that you wish to open it will be 
skipped (along with its script hooks).

## Action Close

Closing luks devices is simply done by invoking

    $ arver --close TARGET

For this action you can define hooks as well. See `Disks` and `Action Open` 
for details.

## Information about targets

To gather various information about the different targets you can invoke

    $ arver -i TARGET
    
Which will display you the current configuration of all devices, as well as
different parameters of the LUKS device and slot usage.

## Managing users

To add another user to one of the disks you need to have the public key of that
user. Just import his key into your gpg keyring. 

If you manage your `.arverdata` in a version controll system, you'll likely
have the key already in `.arverdata/keys/public/USERNAME` where it will be 
imported automatically. 

Granting the user access to any disk is done by invoking the following command:

    $ arver --add-user USERNAME TARGET

For this command to work, you have to trust the gpg key of USERNAME. See 
`man gpg` section --edit-key. You should always verify that you have the correct 
key, e.g. by comparing the fingerprint over a trusted channel. Alternately 
you can run `arver` with `--trust-all` option. 

`arver` will create a random password for the specific user and add it to 
the user-slot on the targeted disks. Furthermore, the password is encrypted
with the public key of the specific user and stored in the data storage under
`arverdata/keys/USERNAME/`.

You now need to pass the new keyfile to the other user. So if you use a version
controll system you should now commit the new keys. The other user should 
follow the paragraph `receiving new keys`.

If you are migrating from an existing LUKS infrastructure and want to add an
initial user to the LUKS device, you will need to use the `--ask-password` option,
to provide an existing password.

To remove the permissions of a certain user you can simply run

    $ arver --del-user USERNAME TARGET

Which will remove the password stored in the LUKS slot of that device. Remember
that you can also invoke this command on a whole hostgroup or even on all your
managed devices (using `-t ALL`). This will help you to quickly and savely
removing any access to encrypted devices of one user immediately amongst the
whole infrastructure.

By design it is not possible to know who has access to which disks by just 
looking at the `arverdata`. All arver keys including the information on which
disks they fit are encrypted with the users public key. So without the 
corresponding private key it is not possible to see the privileges.

You can however display the targets `information` to see which slots are used.
But to do this you need access to the server and the `users` config.

## Receiving new Keys

If you have been granted permission to a certain disk, the first thing you
need to do, is to integrate the new keyfile in your arverdata. If your group
uses a version controll system, you can just pull the arverdata. Otherwise
you copy the received file into `arverdata/keys/USERNAME/`.

Important: The next thing you really should do, is to call
    
    $ arver --refresh TARGET

This will replace the actual luks key on the target once more with a fresh
random key and also collect all your keys into one file. Replacing the key
is important to ensure, that the user who sent you the key, cannot record 
the actual luks key he sent you and use it as a backdoor in the future.

If you use a version controll system to store your `arverdata` you should 
commit the new key.

If you use an old cryptsetup version refresh might not work. In that case you
cannot replace the luks key. Another problem in that case is, that it might become
obvious or at least reconstructable to which devices you have access. To counter 
this problem, do a garbage collection, which will collect all keys into one 
keyfile, with:

    $ arver -g

If you use a version controll system to store your `arverdata` you should do this
always before commiting the `arverdata`.

## SEE ALSO
 
`cryptsetup`(8).
`gnupg`(7).
 
Arver description: <https://tech.immerda.ch/2011/08/arver-distributed-luks-key-management/>

Arver project site: <https://git.codecoop.org/projects/arver/>
 
YAML website: <http://www.yaml.org/>
