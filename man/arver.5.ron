arver(5) -- LUKS on the loose
========================================

## SYNOPSIS

    arver [-u user] [-c arverdata] [OPTIONS] ACTION [USER] [TARGET]
    
## DESCRIPTION

arver is a tool that helps you to manage large amount of encrypted harddisks
over mutliple servers and locations. Furthermore, it helps you to mange
access policies to encrypted harddisks of a bunch of people.

By default `arver` requires a specific action and (for most actions) a 
target to work. 

## OPTIONS

There are some generic command line options:

  * `-u USER`, `--user USER`:
    By default `arver` will read `~/.arver` to get your username. Using `-u USER` you can override it.
  * `-c PATH`, `--config-dir PATH`:
    By default `arver` will assume your data storage in `~/.arverdata`. However,
    if you have multiple data storage or you want to put it to a different
    location you can use the `-c PATH` option.

For more options see `arver -h`

## ACTIONS

The following actions are supported:

  * `--create TARGET`:
    Creates LUKS partitions for `arver` on all targeted disks.
  * `-o`, `--open TARGET`:
    Opens all targeted disks.
  * `--close TARGET`:
    Closes all targeted disks.
  * `-a`, `--add-user USER TARGET`:
    Adds permissions for USER on all targeted disks.
  * `-d`, `--del-user USER TARGET`:
    Removes permissions for USER on all targeted disks.
  * `-i`, `--info TARGET`:
    Display the LUKS configuration of all targeted disks.
  * `-l`, `--list-targets`:
    List all possible targets.
  * `-k`, `--keys TARGET`:
    List available keys for the target.
  * `-g`, `--garbage-collect`:
    Cleans old entries from your `arver` keys.

## TARGETS

All Targets are defined in the `disks` config file. See the section
`Disks` for more details. A complete target looks like this:

    /location_name/host_name/disk_name

But the TARGET option accepts also partial names and lists. E.g 

    location1,location2/host2,host4,disk3

targets all disks at location1, all disks on host2 at location2, all disks on 
host4 and disk3. If any of the provided target names are not unique `arver` will
quit with an error message stating all matching targets.

To run an action on all possible disks use `ALL` as target.

## Working with arver

Working with arver is quite simple and straight forward. Within the next
paragraphs you'll find a detailed overview on the concept of arver,
as well as how to set it up and what the different actions are doing exactly. 

## Concept

To setup arver we only need to define in the corresponding configuration files our
(admin-)users and our disks.
 
Arver's configuration files are contained in a single directory which contains all
the necessary (non-private) information to manage your disks. This directory will
be referred to as `arverdata`. We recommend you to share that directory amongs your
group of admins with a distributed version control system such as git.

Eeach admin will be assigned one global LUKS slot. Arver will use this information
to grant or revoke privileges to other users. See the `Managing users` section
for a detailed descripton of how this is done.

The `arverdata` directory contains the following files and directories:

    keys/            <- contains the users `arver` keyrings
    users            <- yaml file containing all users configuration (see `Users`)
    disks            <- yaml file containing all disk configuration (see `Disks`)
    keys/public/     <- contains gpg public keys of the admins (managed by arver)

### Users

The `user` config file contains all your admins in the following structure:

    foo1: 
      slot: 0
      gpg: BEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFE
    foo2:
      slot 1
      gpg: AFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAFAFFEBEAF

`foo1` is the identifier of one of your admins. `slot` referes to the LUKS slot which
is used for this admin. This has to be a unique number between 0 and 7. If you'd like 
to migrate existing LUKS devices take care to avoid the currently used slot number
(usually 0). `gpg` is the gpg-fingerprint of the public key for `foo1`. We recommend 
to use a dedicated gpg key just for `arver`.

### Disks

The `disks` file contains the following hash tree in yaml notation:

     'hostgroup1':
       'host1':
         'address' : 'host1.example.com'
         'pre_open': 'pre_open_host_script.sh'
         'disk1'   :
           'device'   : 'storage/disk1'
           'post_open': 'post_open_disk_script.sh'
         'disk2'   : 
           'device'   :  'sdb1'
       'host2':
         'address': 'host2.example.com'
         'port'   : '2222'
         'mails'  : 
           'device'  : 'nonraid/mails'
           'pre_open': 'pre_open_disk_script.sh'
     'hostgroup2':
       'host3':
         'address' : 'host3.example.com'
         'username': 'foo'
         'secure'  : 
           'device'  : 'storage/secure'
         
As you can see this allows you to organize your disks and servers in a tree
structure, which will enable you to manage your disks more efficiently within 
the later commands.

`hostgroup1` and `hostgroup2` is just a logical container which can contain any
amount of hosts. You can name them as you like. This is interesting if you have 
for example multiple hosts in one location and you need to quickly recover from 
a power outage from this location. 

Invoking

    arver --list-targets

will present you the tree of the various targets in your `disks` configuration 
file.

`host1`, `host2` and `host3` are identifiers for different hosts. These host-
objects can contain multiple disks and can have further information such as the
`address` of the host or the ssh-`port` number if the ssh daemon is not running
on the standart port.

You can also add script hooks to any host or disk. Those will be run during the
`open` and `close` actions at the appropriate time. The possible options are:
`pre_open`, `pre_close`, `post_open` and `post_close`. 

Any other entry within the hosts-object are actual disks entries of that particular
host. These disks are represented by an identifier and at least a `device` entry
pointing to the actual disk path. So for example the disks on `host1` are:
`/dev/storage/disk1` identified by `disk1` and `/dev/sdb1` idetified by `disk2`.
The prefix `/dev/` is alays added to the disk path.

## Bootstrapping a new arverdata

How do you start and bootstrap a new `arverdata`, so you can use arver for your
storage?

First you need to create the basic structure for your `arverdata`:

    $ mkdir ~/.arverdata # the location is configurable. We use the default one.
    $ gpg --gen-key      # create a dedicated gpg key for arver
    $ vi users           # add your user and the key-id of your new public gpg-key
    $ vi disks           # add your hostgroups, hosts and disks
    $ echo "'username': '<your_arver_username>' > ~/.arver  #set your default user

Then we can create the encrypted harddisk:

## Action Create

To initially create an arver managed LUKS device you first need to add the 
device to the disks config. See above for various examples.
You can create the LUKS device by invoking the following command:

    $ arver --create TARGET 

What's happening behind the scene?

Arver creates a new LUKS device with a random password in your LUKS slot on the 
server. The password is then encrypted with your public key (defined in `users`)
and stored in `arverdata/keys/USERNAME/xxxx`

## Action Open

To open a LUKS device managed by arver you can invoke the `--open` action on any
target:

    $ arver --open TARGET

arver retrieves the password by decrypting the keys in data/keys/YOURUSERNAME
and uses this to open the LUKS device on the server.

See the section `TARGET` on how to open multiple disks at once.

You can define script hooks to be executed before and after the open command.
See `Disks` for details. The hooks are run in the following order:

  * pre_open of host
  *  pre_open of disk1
  *   open disk1
  *  post_open of disk1
  *  pre_open of disk2
  *   open disk2
  *  post_open of disk1
  * post_open of host

Those scripts have to be present at the actual host.

If you don't have a key for any of the disks that you wish to open it will be 
skipped (along with its script hooks).

## Action Close

Closing luks devices is simply done by invoking

    $ arver --close TARGET

For this action you can define hooks as well. See `Disks` and `Action Open` 
for details.

## Managing users

To add another user to one of the disks you need to have the public key of that
user. Just import his key into your gpg keyring. 

If you manage your `.arverdata` in a version controll system, you'll likely
have the key already in `.arverdata/keys/public/USERNAME` where it will be 
imported automatically. 

Granting the user access to any disk is done by invoking the following command:

    $ arver --add-user USERNAME TARGET

For this command to work, you have to trust the gpg key of USERNAME. See 
`man gpg` section --edit-key. You should always verify that you have the correct 
key, e.g. by comparing the fingerprint over a trusted channel. Alternately 
you can run `arver` with `--trust-all` option. 

`arver` will create a random password for the specific user and add it to 
the user-slot on the targeted disks. Furthermore, the password is encrypted
with the public key of the specific user and stored in the data storage under
`arverdata/keys/USERNAME/`.

For the other user to receive those new privileges he has to copy the new
keys to his own `arverdata`. So if you use a version controll system you should
now commit the new keys.

If you are migrating from an existing LUKS infrastructure and want to add an
initial user to the LUKS device, you will need to use the `--ask-password` option,
to provide an existing password.

To remove the permissions of a certain user you can simply run

    $ arver --del-user USERNAME TARGET

Which will remove the password stored in the LUKS slot of that device. Remember
that you can also invoke this command on a whole hostgroup or even on all your
managed devices (using `-t ALL`). This will help you to quickly and savely
removing any access to encrypted devices of one user immediately amongst the
whole infrastructure.

By design it is not possible to know who has access to which disks by just 
looking at the `arverdata`. All arver keys including the information on which
disks they fit are encrypted with the users public key. So without the 
corresponding private key it is not possible to see the privileges.

You can however display the targets `information` to see which slots are used.
But to do this you need access to the server and the `users` config.

## Information about targets

To gather various information about the different targets you can invoke

    $ arver -i TARGET
    
Which will display you the current configuration of all devices, as well as
different parameters of the LUKS device and slot usage.

## Garbage collection

As you might add and remove users to disks or reset access to disks the
amount of generated key files with random passwords might grow and not all
might be needed anymore. Furthermore it is likely that due to various
actions it might be obvious or at least reconstructable to which devices a
certain user might have access.

To address this problem `arver` provides a garbage collection process, which will
rearrange all your own keyfiles. (Only your own as you are not able to read the
others key files.)

You can do that by invoking the following command:

    $ arver -g

If you use a version controll system to store your `arverdata` you should do this
always before commiting the `arverdata`.


## SEE ALSO
 
`cryptsetup`(8).
`gnupg`(7).
 
Arver project site: <https://git.codecoop.org/projects/arver/>
 
YAML website: <http://www.yaml.org/>
